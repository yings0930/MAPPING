{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88700a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial import distance_matrix\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models import InnerProductDecoder, VGAE, GAE\n",
    "from torch_geometric.nn.conv import GCNConv, GINConv, SAGEConv, SGConv, GATConv\n",
    "from torch_geometric.nn import APPNP as APPNPConv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975550d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim:int, hidden_dim:int, output_dim:int, dropout:0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.lin1(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        # x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0965e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hid_dim, dropout)\n",
    "        # self.conv2 = GCNConv(hid_dim, hid_dim, dropout) # delete\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "        # self.dropout = nn.Dropout(dropout) #delete\n",
    "        \n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "        \n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        # x = self.dropout(x)                                 # delete\n",
    "        # x = self.conv2(x, edge_index, edge_weight)            # delete\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGEConv\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hid_dim, normalize=True)\n",
    "        self.conv2 = SAGEConv(hid_dim, hid_dim, normalize=True)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "        self.transition = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.transition(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a27117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPNP\n",
    "class APPNP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super(APPNP, self).__init__()\n",
    "        self.lin1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.lin2 = nn.Linear(hid_dim, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.prop1 = APPNPConv(K=2, alpha=0.1)\n",
    "        #self.prop2 = APPNPConv(K=2, alpha=0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.prop1.reset_parameters()\n",
    "        # self.prop2.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.lin1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.prop1(x, edge_index, edge_weight)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.prop(x, edge_index, edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(nn.Sequential(nn.Linear(in_dim, hid_dim),\n",
    "                             nn.ReLU(), nn.BatchNorm1d(hid_dim),\n",
    "                             nn.Linear(hid_dim, hid_dim)))\n",
    "        #self.conv2 = GINConv(nn.Sequential(nn.Linear(hid_dim, hid_dim),\n",
    "         #                         nn.ReLU()), nn.BatchNorm1d(hid_dim))\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "    \n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        #x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865f832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGConv\n",
    "class SGC(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super(SGC, self).__init__()\n",
    "        self.conv1 = SGConv(in_dim, hid_dim, k=2)\n",
    "        self.conv2 = SGConv(hid_dim, out_dim, k=1)\n",
    "        # self.lin = nn.Linear(hid_dim, out_dim)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118a2aa-7c96-4ba4-9c51-0edb8f5c7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_dim, hid_dim, heads=8)\n",
    "        # On the Pubmed dataset, use `heads` output heads in `conv2`.\n",
    "        # self.conv2 = GATConv(hid_dim * 8, hid_dim, heads=1, concat=False)\n",
    "        self.lin = nn.Linear(hid_dim, out_dim)\n",
    "    \n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # torch.use_deterministic_algorithms(False)\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        # x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
